{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Combined models with loss fucntion and weight options.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2lw74xfHO2v"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8_UkYDpiuo7"
      },
      "source": [
        "**source:** https://www.kaggle.com/awalahmedfime/bacteria-segmentation/edit/run/42535923"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7YSQ1DhHPuw"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRjGbfDri_KX"
      },
      "source": [
        "**Constants**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Select loss\n",
        "model_name = 'resnet50'                                 #Options ['resnet50','resnet101', 'vgg16', 'vgg19']\n",
        "loss_fucntion = 'categorical_crossentropy'             #Options ['categorical_crossentropy', 'dice_coef', 'jaccard_index'] \n",
        "add_weight = 'Yes'                                      #Options ['Yes', 'No'] "
      ],
      "metadata": {
        "id": "SnsnMELJICbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DrmA1ahHPxA"
      },
      "source": [
        "np.random.seed(41)\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "BATCH_SIZE = 16\n",
        "NUM_CLASSES = 3\n",
        "IMG_PATH = '/content/drive/My Drive/Machine Learning/iccv09 3 class/train Image/'\n",
        "MASK_PATH = '/content/drive/My Drive/Machine Learning/iccv09 3 class/mask/'\n",
        "#LABEL_PATH = '../input/bacteria-detection-with-darkfield-microscopy/masks/'\n",
        "IMG_SUB_PATH = '/content/drive/My Drive/Machine Learning/iccv09 3 class/train Image/Image/'\n",
        "MASK_SUB_PATH = '/content/drive/My Drive/Machine Learning/iccv09 3 class/train One hot/One hot/'\n",
        "ONEHOT_MASK = '/content/drive/My Drive/Machine Learning/iccv09 3 class/mask/mask'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKp5KTPjJCr"
      },
      "source": [
        "Mask need to be onehot incoded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G53wPtwPHPz6"
      },
      "source": [
        "\"\"\"mask_files = os.listdir(MASK_SUB_PATH)\n",
        "for mf in tqdm (mask_files):\n",
        "    mask_img = cv2.imread(os.path.join(MASK_SUB_PATH, mf))\n",
        "    mask_img = mask_img/255\n",
        "    cv2.imwrite(os.path.join(ONEHOT_MASK, mf), mask_img)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y-TAbYZAtZF"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEZYpb1oBsdg"
      },
      "source": [
        "**source:** https://github.com/keras-team/keras/issues/3059#issuecomment-364787723"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfXZ1NUnHP9y"
      },
      "source": [
        "training_generation_args = dict(\n",
        "     #width_shift_range=0.3,\n",
        "     #height_shift_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    #vertical_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    validation_split=0.1,\n",
        ")\n",
        "train_image_datagen = ImageDataGenerator(**training_generation_args)\n",
        "train_label_datagen = ImageDataGenerator(**training_generation_args)\n",
        "\n",
        "# data load\n",
        "training_image_generator = train_image_datagen.flow_from_directory(\n",
        "    IMG_PATH,\n",
        "    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    class_mode=None,\n",
        "    subset='training',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=1\n",
        ")\n",
        "training_label_generator = train_label_datagen.flow_from_directory(\n",
        "    MASK_PATH,\n",
        "    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    class_mode=None,\n",
        "    subset='training',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    # color_mode='grayscale',\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "\n",
        "# validation data load\n",
        "validation_image_generator = train_image_datagen.flow_from_directory(\n",
        "    IMG_PATH,\n",
        "    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    class_mode=None,\n",
        "    subset='validation',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=1\n",
        ")\n",
        "validation_label_generator = train_label_datagen.flow_from_directory(\n",
        "    MASK_PATH,\n",
        "    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "    class_mode=None,\n",
        "    subset='validation',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    # color_mode='grayscale',\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "train_generator = zip(training_image_generator, training_label_generator)\n",
        "validation_generator = zip(validation_image_generator, validation_label_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYWWwsSENXV"
      },
      "source": [
        "**Class imblanace**\n",
        "source:  https://stackoverflow.com/questions/52123670 \n",
        "Keras model loss_weight: Keras - compile method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4BG7ohkEKbV"
      },
      "source": [
        "loss_weights = {\n",
        "    0: 0,\n",
        "    1: 0,\n",
        "    2:0\n",
        "}\n",
        "mask_files = os.listdir(ONEHOT_MASK)\n",
        "for mf in tqdm(mask_files):\n",
        "    mask_img = cv2.imread(os.path.join(ONEHOT_MASK, mf))\n",
        "    classes = tf.argmax(mask_img, axis=-1).numpy()\n",
        "    class_counts = np.unique(classes, return_counts=True)\n",
        "    \n",
        "    for c in range(len(class_counts[0])):\n",
        "        loss_weights[class_counts[0][c]] += class_counts[1][c]\n",
        "\n",
        "print(loss_weights)\n",
        "\n",
        "total = sum(loss_weights.values())\n",
        "for cl, v in loss_weights.items():\n",
        "    # do inverse\n",
        "    loss_weights[cl] = total / (v*3)\n",
        "    \n",
        "loss_weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUj9mxbpSYWP"
      },
      "source": [
        "Create a modifier that is the same shape as output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q6JP3NbHQA6"
      },
      "source": [
        "w = [[loss_weights[0], loss_weights[1], loss_weights[2]]] * IMAGE_WIDTH\n",
        "h = [w] * IMAGE_HEIGHT\n",
        "loss_mod = np.array(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0hfJx7oScZf"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51jt7xcfHQDT"
      },
      "source": [
        "\n",
        "def AtrousSpatialPyramidPooling(input_shape):\n",
        "\n",
        "  inputs = tf.keras.Input(input_shape[-3:]);\n",
        "  # global pooling\n",
        "  results = tf.keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, [1,2], keepdims = True))(inputs);\n",
        "  results = tf.keras.layers.Conv2D(256, kernel_size = (1,1), padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  results = tf.keras.layers.ReLU()(results);\n",
        "  pool = tf.keras.layers.UpSampling2D(size = (input_shape[-3] // results.shape[1], input_shape[-2] // results.shape[2]), interpolation = 'bilinear')(results);\n",
        "  results = tf.keras.layers.Conv2D(256, kernel_size = (1,1), dilation_rate = 1, padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(inputs);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  dilated_1 = tf.keras.layers.ReLU()(results);\n",
        "  results = tf.keras.layers.Conv2D(256, kernel_size = (3,3), dilation_rate = 6, padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(inputs);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  dilated_6 = tf.keras.layers.ReLU()(results);\n",
        "  results = tf.keras.layers.Conv2D(256, kernel_size = (3,3), dilation_rate = 12, padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(inputs);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  dilated_12 = tf.keras.layers.ReLU()(results);\n",
        "  results = tf.keras.layers.Conv2D(256, kernel_size = (3,3), dilation_rate = 18, padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(inputs);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  dilated_18 = tf.keras.layers.ReLU()(results);\n",
        "  results = tf.keras.layers.Concatenate(axis = -1)([dilated_1, dilated_6, dilated_12, dilated_18, pool]);\n",
        "  results = tf.keras.layers.Conv2D(256, kernel_size = (1,1), dilation_rate = 1, padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  results = tf.keras.layers.ReLU()(results);\n",
        "  return tf.keras.Model(inputs = inputs, outputs = results);\n",
        "\n",
        "input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
        "\n",
        "inputs = tf.keras.Input(input_shape[-3:]);\n",
        "if model_name == 'resnet101':\n",
        "  resnet101 = tf.keras.applications.ResNet101(input_tensor = inputs, weights = 'imagenet', include_top = False);\n",
        "  results = resnet101.get_layer('conv4_block23_2_relu').output;\n",
        "  results = AtrousSpatialPyramidPooling(results.shape[-3:])(results);\n",
        "  a = tf.keras.layers.UpSampling2D(size = (input_shape[-3] // 4 // results.shape[1], input_shape[-2] // 4 // results.shape[2]), interpolation = 'bilinear')(results);\n",
        "  results = resnet101.get_layer('conv2_block3_2_relu').output;\n",
        "  results = tf.keras.layers.Conv2D(48, kernel_size = (1,1), padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  b = tf.keras.layers.ReLU()(results);\n",
        "elif model_name == 'vgg16':\n",
        "  vgg16 = tf.keras.applications.VGG16(input_tensor = inputs, weights = 'imagenet', include_top = False);\n",
        "  # a.shape = (batch, height // 4, width // 4, 256)\n",
        "  results = vgg16.get_layer('block5_conv3').output;\n",
        "  results = AtrousSpatialPyramidPooling(results.shape[-3:])(results);\n",
        "  a = tf.keras.layers.UpSampling2D(size = (input_shape[-3] // 4 // results.shape[1], input_shape[-2] // 4 // results.shape[2]), interpolation = 'bilinear')(results);\n",
        "  # b.shape = (batch, height // 4, width // 4, 48)\n",
        "  results = vgg16.get_layer('block3_conv3').output;\n",
        "  results = tf.keras.layers.Conv2D(48, kernel_size = (1,1), padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  b = tf.keras.layers.ReLU()(results);\n",
        "elif model_name == 'vgg19':\n",
        "  vgg19 = tf.keras.applications.VGG19(input_tensor = inputs, weights = 'imagenet', include_top = False);\n",
        "  results = vgg19.get_layer('block5_conv4').output;\n",
        "  results = AtrousSpatialPyramidPooling(results.shape[-3:])(results);\n",
        "  a = tf.keras.layers.UpSampling2D(size = (input_shape[-3] // 4 // results.shape[1], input_shape[-2] // 4 // results.shape[2]), interpolation = 'bilinear')(results);\n",
        "  results = vgg19.get_layer('block3_conv4').output;\n",
        "  results = tf.keras.layers.Conv2D(48, kernel_size = (1,1), padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  b = tf.keras.layers.ReLU()(results);\n",
        "else:\n",
        "  model_name = 'resnet50'\n",
        "  resnet50 = tf.keras.applications.ResNet50(input_tensor = inputs, weights = 'imagenet', include_top = False);\n",
        "  results = resnet50.get_layer('conv4_block6_2_relu').output;\n",
        "  results = AtrousSpatialPyramidPooling(results.shape[-3:])(results);\n",
        "  a = tf.keras.layers.UpSampling2D(size = (input_shape[-3] // 4 // results.shape[1], input_shape[-2] // 4 // results.shape[2]), interpolation = 'bilinear')(results);\n",
        "  results = resnet50.get_layer('conv2_block3_2_relu').output;\n",
        "  results = tf.keras.layers.Conv2D(48, kernel_size = (1,1), padding = 'same', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "  results = tf.keras.layers.BatchNormalization()(results);\n",
        "  b = tf.keras.layers.ReLU()(results);\n",
        "results = tf.keras.layers.Concatenate(axis = -1)([a, b]);\n",
        "results = tf.keras.layers.Conv2D(256, kernel_size = (3,3), padding = 'same', activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "results = tf.keras.layers.BatchNormalization()(results);\n",
        "results = tf.keras.layers.ReLU()(results);\n",
        "results = tf.keras.layers.Conv2D(32, kernel_size = (3,3), padding = 'same', activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal(), use_bias = False)(results);\n",
        "results = tf.keras.layers.BatchNormalization()(results);\n",
        "results = tf.keras.layers.ReLU()(results);\n",
        "results = tf.keras.layers.UpSampling2D(size = (input_shape[-3] // results.shape[1], input_shape[-2] // results.shape[2]), interpolation = 'bilinear')(results);\n",
        "output1 = tf.keras.layers.Conv2D(3, kernel_size = (1,1), padding = 'same', activation = 'softmax')(results);\n",
        "\n",
        "model = tf.keras.Model(inputs = [inputs], outputs = [output1])\n",
        "#model = DeeplabV3Plus((274,256,256,3),9);\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctYhe2_SnNQH"
      },
      "source": [
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "oJUVV0PoE206"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)"
      ],
      "metadata": {
        "id": "BMII5D8vE5O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_index(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    #return (1 - jac) * smooth\n",
        "    return jac\n",
        "\n",
        "def jaccard_index_loss(y_true, y_pred, smooth=1):\n",
        "    return (1 - jaccard_index(y_true, y_pred)) * smooth"
      ],
      "metadata": {
        "id": "b7PVeWQHGS0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDMHK7W_HQMh"
      },
      "source": [
        "if loss_fucntion == 'dice_coef' and add_weight == 'Yes':\n",
        "  # Dice Coefficient loss with weight\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=dice_coef_loss,\n",
        "                metrics=[dice_coef],\n",
        "                loss_weights=loss_mod)\n",
        "\n",
        "elif loss_fucntion == 'dice_coef' and add_weight == 'No':\n",
        "  # Dice Coefficient loss without weight\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=dice_coef_loss,\n",
        "                metrics=[dice_coef])\n",
        "\n",
        "elif loss_fucntion == 'jaccard_index' and add_weight == 'Yes':\n",
        "  # Jaccard Index loss with weight\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=jaccard_index_loss,\n",
        "                metrics=[jaccard_index],\n",
        "                loss_weights=loss_mod)\n",
        "\n",
        "elif loss_fucntion == 'jaccard_index' and add_weight == 'No':\n",
        "  # Jaccard Index loss without weight\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=jaccard_index_loss,\n",
        "                metrics=[jaccard_index])\n",
        "\n",
        "elif loss_fucntion == 'categorical_crossentropy' and add_weight == 'Yes':\n",
        "  # Categorical crossentropy loss with weight\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'],\n",
        "                loss_weights=loss_mod)\n",
        "\n",
        "else:\n",
        "  # Categorical crossentropy loss without weight\n",
        "  loss_fucntion = 'categorical_crossentropy'  \n",
        "  add_weight == 'No'\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXAt4cufHQQX"
      },
      "source": [
        "ACCURACY_THRESHOLD = 0.0\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        global ACCURACY_THRESHOLD\n",
        "        if(logs.get('val_accuracy') > ACCURACY_THRESHOLD ):\n",
        "            ACCURACY_THRESHOLD = logs.get('val_accuracy')\n",
        "            model.save(\"/content/drive/MyDrive/Machine Learning/iccv09 3 class/\"+model_name+\"_\"+loss_fucntion+\"_loss_\"+add_weight+\".h5\")\n",
        "callbacks = myCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PALIZmwHQKT"
      },
      "source": [
        "model_history = model.fit(train_generator,\n",
        "                          epochs=1,\n",
        "                          steps_per_epoch=training_image_generator.samples // BATCH_SIZE,\n",
        "                          #shuffle=True,\n",
        "                          validation_data=validation_generator,\n",
        "                          validation_steps=validation_image_generator.samples // BATCH_SIZE,\n",
        "                          callbacks=[callbacks])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NSyIl3OpTkd"
      },
      "source": [
        "#save history\n",
        "np.save(\"/content/drive/MyDrive/Machine Learning/iccv09 3 class/\"+model_name+\"_\"+loss_fucntion+\"_loss_\"+add_weight+\".npy\",model_history.history)\n",
        "hist_df = pd.DataFrame(model_history.history) \n",
        "hist_csv_file = \"/content/drive/MyDrive/Machine Learning/iccv09 3 class/\"+model_name+\"_\"+loss_fucntion+\"_loss_\"+add_weight+\".csv\"\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE3TF7QNzWxy"
      },
      "source": [
        "#Reload model\n",
        "modelFile = \"/content/drive/MyDrive/Machine Learning/iccv09 3 class/\"+model_name+\"_\"+loss_fucntion+\"_loss_\"+add_weight+\".h5\"\n",
        "\n",
        "if loss_fucntion == 'jaccard_index':\n",
        "  model = keras.models.load_model(modelFile, custom_objects={'jaccard_index_loss': jaccard_index_loss, 'jaccard_index':jaccard_index})\n",
        "elif loss_fucntion == 'dice_coef':\n",
        "  model = keras.models.load_model(modelFile, custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef})\n",
        "else:\n",
        "  model = keras.models.load_model(modelFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "punX5uHk0gri"
      },
      "source": [
        "test1 = \"/content/drive/My Drive/Machine Learning/iccv09 3 class/Out pic\"\n",
        "out = \"/content/drive/MyDrive/Machine Learning/\"+model_name+\"_\"+loss_fucntion+\"_loss_\"+add_weight+\"/\"\n",
        "mask_files = os.listdir(test1)\n",
        "for mf in tqdm (mask_files):\n",
        "    img = cv2.imread(os.path.join(test1, mf) )\n",
        "    #print(img.shape)\n",
        "    img =  cv2.resize(img, (256, 256))\n",
        "    img=np.expand_dims(img, 0)\n",
        "    img = model.predict(img)\n",
        "    img =  np.squeeze(img)\n",
        "    img1 = np.zeros(( IMAGE_HEIGHT , IMAGE_WIDTH, 3), dtype= np.int)\n",
        "    for i in range(256):\n",
        "      for j in range(256):\n",
        "          img1[i][j][np.argmax(img[i][j])]=1\n",
        "    cv2.imwrite(os.path.join(out, mf), img1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d6rngEfAOC0"
      },
      "source": [
        "\n",
        "mask_files = os.listdir(out)\n",
        "for mf in tqdm (mask_files):\n",
        "  img = cv2.imread(os.path.join(out, mf) )\n",
        "  img = img*255\n",
        "  cv2.imwrite(os.path.join(out, mf), cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_p = \"/content/drive/My Drive/Machine Learning/iccv09 3 class/test Image/\"\n",
        "test_m = \"/content/drive/My Drive/Machine Learning/iccv09 3 class/test one hot/\"\n",
        "\n",
        "\n",
        "tst_fies = os.listdir(test_m)\n",
        "tst_fies.sort()\n",
        "test= np.zeros((len(tst_fies), IMAGE_HEIGHT , IMAGE_WIDTH, 3), dtype= np.bool)\n",
        "for n, mf in tqdm(enumerate(tst_fies), total=len(tst_fies)):\n",
        "  img = cv2.imread(os.path.join(test_m, mf) )\n",
        "  img = cv2.resize(img, (256, 256))\n",
        "  img = img / 255\n",
        "  test[n] = img\n",
        "\n",
        "\n",
        "acc = []\n",
        "pre = []\n",
        "rec = []\n",
        "f_me = []\n",
        "test_files = os.listdir(test_p)\n",
        "test_files.sort()\n",
        "for n, mf in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "  img = cv2.imread(os.path.join(test_p, mf) )\n",
        "  img =  cv2.resize(img, (256, 256))\n",
        "  img=np.expand_dims(img, 0)\n",
        "  img = model.predict(img)\n",
        "  img =  np.squeeze(img)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = img > .5\n",
        "  test1 = test[n]\n",
        "  TP = tf.math.count_nonzero(img * test1)\n",
        "  TN = tf.math.count_nonzero((img - 1) * (test1 - 1))\n",
        "  FP = tf.math.count_nonzero(img * (test1 - 1))\n",
        "  FN = tf.math.count_nonzero((img - 1) * test1)\n",
        "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1 = 2 * precision * recall / (precision + recall)\n",
        "  acc.append(accuracy)\n",
        "  pre.append(precision)\n",
        "  rec.append(recall)\n",
        "  f_me.append(f1)\n"
      ],
      "metadata": {
        "id": "1_XxTbdc64vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWwlCTKQ0F2P"
      },
      "source": [
        "print(\"Mean\")\n",
        "\n",
        "print(sum(rec)/len(tst_fies))\n",
        "print(sum(pre)/len(tst_fies))\n",
        "print(sum(acc)/len(tst_fies))\n",
        "print(sum(f_me)/len(tst_fies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1Pr7FwNr-cD"
      },
      "source": [
        "print(\"SD\")\n",
        "\n",
        "print(np.std(rec))\n",
        "print(np.std(pre))\n",
        "print(np.std(acc))\n",
        "print(np.std(f_me))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}